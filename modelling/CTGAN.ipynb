{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrC6ioe9eyi7",
        "outputId": "1f7b30ac-5f82-49a4-b0e7-432e0c400f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "!pip install ctgan"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ctgan\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/e6/b744b740fb9636f726f0e8272fc25beb3340c6b1cd5385a9dd87f0316c24/ctgan-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn<0.23,>=0.21 in /usr/local/lib/python3.6/dist-packages (from ctgan) (0.22.2.post1)\n",
            "Collecting pandas<0.26,>=0.24\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision<1,>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from ctgan) (0.8.1+cu101)\n",
            "Requirement already satisfied: numpy<2,>=1.17.4 in /usr/local/lib/python3.6/dist-packages (from ctgan) (1.18.5)\n",
            "Requirement already satisfied: torch<2,>=1.0 in /usr/local/lib/python3.6/dist-packages (from ctgan) (1.7.0+cu101)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.23,>=0.21->ctgan) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.23,>=0.21->ctgan) (0.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas<0.26,>=0.24->ctgan) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<0.26,>=0.24->ctgan) (2018.9)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision<1,>=0.4.2->ctgan) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.0->ctgan) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.0->ctgan) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.0->ctgan) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas<0.26,>=0.24->ctgan) (1.15.0)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas, ctgan\n",
            "  Found existing installation: pandas 1.1.4\n",
            "    Uninstalling pandas-1.1.4:\n",
            "      Successfully uninstalled pandas-1.1.4\n",
            "Successfully installed ctgan-0.2.1 pandas-0.25.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apD2NkCrfq1i"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HKIQho5fqvG"
      },
      "source": [
        "data = pd.read_csv('forestImputedTrain.csv')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH6YZ_Bxfqjj",
        "outputId": "ee2e30ce-a60e-42d4-f990-865da7c3aefd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEzlwhA7gThn"
      },
      "source": [
        "train_y = pd.read_csv('train_y.csv')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWUsdgBvgmxG",
        "outputId": "74909efb-a84b-425e-9eff-c1184f7aefd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_y)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CivBfRlcgolm"
      },
      "source": [
        "drop_indices = train_y[train_y['Label'].isnull()].index\n",
        "keep_indices = [i for i in train_y.index if i not in drop_indices]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91E8WAm2grCO",
        "outputId": "be765126-4e09-4ea5-a8b6-e578c7548159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train = data.loc[keep_indices]\n",
        "train['Age'] = train['Age'].astype('category')\n",
        "train['Occupation type'] = train['Occupation type'].astype('category')\n",
        "train['Loan type'] = train['Loan type'].astype('category')\n",
        "train['Label'] = train['Label'].astype('category')\n",
        "train.dtypes"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID                  float64\n",
              "Expense             float64\n",
              "Income              float64\n",
              "Loan type          category\n",
              "Occupation type    category\n",
              "Age                category\n",
              "Score1              float64\n",
              "Score2              float64\n",
              "Score3              float64\n",
              "Score4              float64\n",
              "Score5              float64\n",
              "Label              category\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz-LfhnRgsPu"
      },
      "source": [
        "discrete_columns = ['Loan type', 'Occupation type', 'Age', 'Label']"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICyxQZ3gvtl"
      },
      "source": [
        "from ctgan import CTGANSynthesizer\n",
        "\n",
        "ctgan = CTGANSynthesizer()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDyf0nzIgygk",
        "outputId": "b124efed-398f-4584-9cb5-60c733133f12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ctgan.fit(train, discrete_columns)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss G: -0.4002, Loss D: -0.0225\n",
            "Epoch 2, Loss G: -1.2303, Loss D: 0.0273\n",
            "Epoch 3, Loss G: -1.4824, Loss D: 0.0494\n",
            "Epoch 4, Loss G: -1.4380, Loss D: -0.0660\n",
            "Epoch 5, Loss G: -1.4740, Loss D: 0.0185\n",
            "Epoch 6, Loss G: -1.4384, Loss D: -0.0678\n",
            "Epoch 7, Loss G: -1.5034, Loss D: -0.1013\n",
            "Epoch 8, Loss G: -1.3978, Loss D: -0.0198\n",
            "Epoch 9, Loss G: -1.4810, Loss D: 0.0729\n",
            "Epoch 10, Loss G: -1.4279, Loss D: -0.0468\n",
            "Epoch 11, Loss G: -1.6187, Loss D: -0.0772\n",
            "Epoch 12, Loss G: -1.6191, Loss D: -0.0559\n",
            "Epoch 13, Loss G: -1.5913, Loss D: -0.0761\n",
            "Epoch 14, Loss G: -1.4213, Loss D: -0.0736\n",
            "Epoch 15, Loss G: -1.3564, Loss D: -0.0833\n",
            "Epoch 16, Loss G: -1.4049, Loss D: -0.0031\n",
            "Epoch 17, Loss G: -1.3093, Loss D: -0.0932\n",
            "Epoch 18, Loss G: -1.3095, Loss D: 0.0766\n",
            "Epoch 19, Loss G: -1.0705, Loss D: -0.1046\n",
            "Epoch 20, Loss G: -0.9827, Loss D: -0.1669\n",
            "Epoch 21, Loss G: -0.6949, Loss D: -0.2051\n",
            "Epoch 22, Loss G: -0.6107, Loss D: -0.2564\n",
            "Epoch 23, Loss G: -0.5882, Loss D: -0.1712\n",
            "Epoch 24, Loss G: -0.8318, Loss D: 0.3389\n",
            "Epoch 25, Loss G: -0.9293, Loss D: -0.1310\n",
            "Epoch 26, Loss G: -0.8375, Loss D: -0.0835\n",
            "Epoch 27, Loss G: -0.9763, Loss D: -0.0798\n",
            "Epoch 28, Loss G: -0.8098, Loss D: -0.0271\n",
            "Epoch 29, Loss G: -0.6220, Loss D: -0.0511\n",
            "Epoch 30, Loss G: -0.6655, Loss D: -0.0966\n",
            "Epoch 31, Loss G: -0.4219, Loss D: -0.2688\n",
            "Epoch 32, Loss G: -0.6759, Loss D: -0.0057\n",
            "Epoch 33, Loss G: -0.3861, Loss D: -0.0908\n",
            "Epoch 34, Loss G: -0.4498, Loss D: -0.0401\n",
            "Epoch 35, Loss G: -0.3541, Loss D: -0.1123\n",
            "Epoch 36, Loss G: -0.3185, Loss D: -0.2638\n",
            "Epoch 37, Loss G: -0.2626, Loss D: -0.0664\n",
            "Epoch 38, Loss G: -0.2575, Loss D: 0.0604\n",
            "Epoch 39, Loss G: -0.2849, Loss D: -0.2841\n",
            "Epoch 40, Loss G: -0.1470, Loss D: -0.1891\n",
            "Epoch 41, Loss G: -0.1122, Loss D: -0.3622\n",
            "Epoch 42, Loss G: -0.2234, Loss D: -0.0732\n",
            "Epoch 43, Loss G: -0.2934, Loss D: -0.1515\n",
            "Epoch 44, Loss G: -0.3619, Loss D: -0.0681\n",
            "Epoch 45, Loss G: -0.2125, Loss D: -0.0444\n",
            "Epoch 46, Loss G: -0.0506, Loss D: -0.0137\n",
            "Epoch 47, Loss G: -0.0105, Loss D: -0.0987\n",
            "Epoch 48, Loss G: 0.0798, Loss D: 0.0969\n",
            "Epoch 49, Loss G: 0.4161, Loss D: -0.0593\n",
            "Epoch 50, Loss G: 0.3307, Loss D: -0.2151\n",
            "Epoch 51, Loss G: 0.2804, Loss D: -0.1592\n",
            "Epoch 52, Loss G: 0.3127, Loss D: 0.1736\n",
            "Epoch 53, Loss G: 0.1594, Loss D: -0.0432\n",
            "Epoch 54, Loss G: 0.0416, Loss D: 0.0269\n",
            "Epoch 55, Loss G: 0.2977, Loss D: -0.3116\n",
            "Epoch 56, Loss G: 0.4724, Loss D: -0.2690\n",
            "Epoch 57, Loss G: 0.5317, Loss D: 0.0490\n",
            "Epoch 58, Loss G: 0.3635, Loss D: 0.1051\n",
            "Epoch 59, Loss G: 0.4404, Loss D: -0.2323\n",
            "Epoch 60, Loss G: 0.0869, Loss D: -0.1236\n",
            "Epoch 61, Loss G: 0.2446, Loss D: -0.1040\n",
            "Epoch 62, Loss G: 0.0254, Loss D: -0.0440\n",
            "Epoch 63, Loss G: 0.4201, Loss D: -0.1911\n",
            "Epoch 64, Loss G: 0.1895, Loss D: 0.0238\n",
            "Epoch 65, Loss G: 0.4911, Loss D: -0.2912\n",
            "Epoch 66, Loss G: 0.3485, Loss D: -0.1287\n",
            "Epoch 67, Loss G: 0.3975, Loss D: -0.1483\n",
            "Epoch 68, Loss G: 0.1460, Loss D: -0.2048\n",
            "Epoch 69, Loss G: 0.1917, Loss D: -0.1444\n",
            "Epoch 70, Loss G: 0.2875, Loss D: -0.2444\n",
            "Epoch 71, Loss G: 0.2590, Loss D: -0.3231\n",
            "Epoch 72, Loss G: 0.3506, Loss D: 0.0673\n",
            "Epoch 73, Loss G: 0.2187, Loss D: -0.2322\n",
            "Epoch 74, Loss G: 0.1645, Loss D: -0.1203\n",
            "Epoch 75, Loss G: 0.4476, Loss D: -0.1474\n",
            "Epoch 76, Loss G: 0.2220, Loss D: 0.0822\n",
            "Epoch 77, Loss G: 0.1503, Loss D: -0.1821\n",
            "Epoch 78, Loss G: 0.2009, Loss D: -0.0589\n",
            "Epoch 79, Loss G: 0.1213, Loss D: -0.1214\n",
            "Epoch 80, Loss G: 0.1685, Loss D: 0.0416\n",
            "Epoch 81, Loss G: 0.0343, Loss D: 0.1286\n",
            "Epoch 82, Loss G: 0.0115, Loss D: -0.1137\n",
            "Epoch 83, Loss G: 0.1039, Loss D: -0.0669\n",
            "Epoch 84, Loss G: -0.1929, Loss D: 0.0004\n",
            "Epoch 85, Loss G: -0.0651, Loss D: -0.2202\n",
            "Epoch 86, Loss G: -0.3147, Loss D: 0.0784\n",
            "Epoch 87, Loss G: -0.2275, Loss D: -0.0934\n",
            "Epoch 88, Loss G: -0.2023, Loss D: -0.1332\n",
            "Epoch 89, Loss G: -0.2925, Loss D: 0.0655\n",
            "Epoch 90, Loss G: -0.1046, Loss D: -0.1002\n",
            "Epoch 91, Loss G: -0.2735, Loss D: -0.0576\n",
            "Epoch 92, Loss G: -0.3435, Loss D: -0.0816\n",
            "Epoch 93, Loss G: -0.1303, Loss D: -0.1085\n",
            "Epoch 94, Loss G: -0.1955, Loss D: 0.0083\n",
            "Epoch 95, Loss G: -0.2780, Loss D: -0.0604\n",
            "Epoch 96, Loss G: -0.1901, Loss D: -0.1220\n",
            "Epoch 97, Loss G: -0.1917, Loss D: -0.1280\n",
            "Epoch 98, Loss G: -0.3046, Loss D: 0.0338\n",
            "Epoch 99, Loss G: -0.1636, Loss D: -0.1151\n",
            "Epoch 100, Loss G: -0.2472, Loss D: 0.0388\n",
            "Epoch 101, Loss G: -0.0876, Loss D: -0.0052\n",
            "Epoch 102, Loss G: -0.2815, Loss D: 0.1026\n",
            "Epoch 103, Loss G: -0.3470, Loss D: -0.0160\n",
            "Epoch 104, Loss G: -0.2310, Loss D: -0.0812\n",
            "Epoch 105, Loss G: -0.1537, Loss D: -0.0376\n",
            "Epoch 106, Loss G: -0.3686, Loss D: 0.0080\n",
            "Epoch 107, Loss G: -0.1504, Loss D: -0.1220\n",
            "Epoch 108, Loss G: -0.1041, Loss D: -0.1076\n",
            "Epoch 109, Loss G: -0.3833, Loss D: -0.3392\n",
            "Epoch 110, Loss G: -0.3560, Loss D: 0.0476\n",
            "Epoch 111, Loss G: -0.1143, Loss D: -0.0959\n",
            "Epoch 112, Loss G: -0.0404, Loss D: 0.0285\n",
            "Epoch 113, Loss G: -0.2518, Loss D: 0.2153\n",
            "Epoch 114, Loss G: -0.4244, Loss D: -0.0729\n",
            "Epoch 115, Loss G: -0.5803, Loss D: -0.1515\n",
            "Epoch 116, Loss G: -0.2367, Loss D: -0.2108\n",
            "Epoch 117, Loss G: -0.3310, Loss D: -0.0023\n",
            "Epoch 118, Loss G: -0.1238, Loss D: 0.0536\n",
            "Epoch 119, Loss G: -0.6380, Loss D: -0.0934\n",
            "Epoch 120, Loss G: -0.2679, Loss D: -0.2551\n",
            "Epoch 121, Loss G: -0.2709, Loss D: -0.0837\n",
            "Epoch 122, Loss G: -0.3104, Loss D: -0.1920\n",
            "Epoch 123, Loss G: -0.3919, Loss D: -0.0936\n",
            "Epoch 124, Loss G: -0.5644, Loss D: -0.1275\n",
            "Epoch 125, Loss G: -0.1960, Loss D: 0.0085\n",
            "Epoch 126, Loss G: -0.5219, Loss D: -0.0946\n",
            "Epoch 127, Loss G: -0.3684, Loss D: -0.2158\n",
            "Epoch 128, Loss G: -0.4483, Loss D: -0.2141\n",
            "Epoch 129, Loss G: -0.7115, Loss D: 0.1156\n",
            "Epoch 130, Loss G: -0.5847, Loss D: -0.0575\n",
            "Epoch 131, Loss G: -0.4902, Loss D: 0.0065\n",
            "Epoch 132, Loss G: -0.6184, Loss D: 0.0101\n",
            "Epoch 133, Loss G: -0.3239, Loss D: -0.1087\n",
            "Epoch 134, Loss G: -0.7359, Loss D: -0.1448\n",
            "Epoch 135, Loss G: -0.4482, Loss D: -0.0401\n",
            "Epoch 136, Loss G: -0.7898, Loss D: 0.0202\n",
            "Epoch 137, Loss G: -0.2577, Loss D: -0.1031\n",
            "Epoch 138, Loss G: -0.4222, Loss D: -0.0421\n",
            "Epoch 139, Loss G: -0.4407, Loss D: -0.0942\n",
            "Epoch 140, Loss G: -0.3026, Loss D: -0.0319\n",
            "Epoch 141, Loss G: -0.6018, Loss D: -0.1285\n",
            "Epoch 142, Loss G: -0.6344, Loss D: -0.1265\n",
            "Epoch 143, Loss G: -0.4648, Loss D: -0.0040\n",
            "Epoch 144, Loss G: -0.6598, Loss D: -0.0093\n",
            "Epoch 145, Loss G: -0.6245, Loss D: -0.0544\n",
            "Epoch 146, Loss G: -0.4778, Loss D: -0.1411\n",
            "Epoch 147, Loss G: -0.5885, Loss D: -0.0497\n",
            "Epoch 148, Loss G: -0.4103, Loss D: -0.0997\n",
            "Epoch 149, Loss G: -0.6024, Loss D: -0.0315\n",
            "Epoch 150, Loss G: -0.5333, Loss D: -0.0550\n",
            "Epoch 151, Loss G: -0.4745, Loss D: -0.0557\n",
            "Epoch 152, Loss G: -0.6361, Loss D: -0.0358\n",
            "Epoch 153, Loss G: -0.4999, Loss D: -0.0608\n",
            "Epoch 154, Loss G: -0.8118, Loss D: -0.1234\n",
            "Epoch 155, Loss G: -0.4010, Loss D: -0.2183\n",
            "Epoch 156, Loss G: -0.6824, Loss D: -0.0147\n",
            "Epoch 157, Loss G: -0.4649, Loss D: -0.0495\n",
            "Epoch 158, Loss G: -0.4946, Loss D: -0.1119\n",
            "Epoch 159, Loss G: -0.6037, Loss D: -0.0684\n",
            "Epoch 160, Loss G: -0.5922, Loss D: -0.1890\n",
            "Epoch 161, Loss G: -0.6420, Loss D: -0.1926\n",
            "Epoch 162, Loss G: -0.7378, Loss D: -0.1039\n",
            "Epoch 163, Loss G: -0.5340, Loss D: -0.0861\n",
            "Epoch 164, Loss G: -0.3183, Loss D: -0.0925\n",
            "Epoch 165, Loss G: -0.5250, Loss D: -0.1846\n",
            "Epoch 166, Loss G: -0.4871, Loss D: 0.0938\n",
            "Epoch 167, Loss G: -0.4756, Loss D: 0.0700\n",
            "Epoch 168, Loss G: -0.4707, Loss D: -0.0589\n",
            "Epoch 169, Loss G: -0.4335, Loss D: -0.1477\n",
            "Epoch 170, Loss G: -0.3913, Loss D: -0.2819\n",
            "Epoch 171, Loss G: -0.2952, Loss D: -0.1229\n",
            "Epoch 172, Loss G: -0.1315, Loss D: -0.1005\n",
            "Epoch 173, Loss G: -0.5657, Loss D: -0.0145\n",
            "Epoch 174, Loss G: -0.3732, Loss D: -0.0933\n",
            "Epoch 175, Loss G: -0.4867, Loss D: 0.0262\n",
            "Epoch 176, Loss G: -0.4465, Loss D: -0.0698\n",
            "Epoch 177, Loss G: -0.7093, Loss D: 0.0127\n",
            "Epoch 178, Loss G: -0.2169, Loss D: -0.0330\n",
            "Epoch 179, Loss G: -0.5876, Loss D: -0.2054\n",
            "Epoch 180, Loss G: -0.2952, Loss D: -0.1318\n",
            "Epoch 181, Loss G: -0.5662, Loss D: -0.1066\n",
            "Epoch 182, Loss G: -0.5501, Loss D: -0.1519\n",
            "Epoch 183, Loss G: -0.6254, Loss D: 0.1309\n",
            "Epoch 184, Loss G: -0.2786, Loss D: -0.2026\n",
            "Epoch 185, Loss G: -0.5314, Loss D: 0.0392\n",
            "Epoch 186, Loss G: -0.1665, Loss D: -0.0202\n",
            "Epoch 187, Loss G: -0.5738, Loss D: -0.1845\n",
            "Epoch 188, Loss G: -0.3423, Loss D: -0.1491\n",
            "Epoch 189, Loss G: 0.0382, Loss D: -0.2280\n",
            "Epoch 190, Loss G: -0.4093, Loss D: 0.0141\n",
            "Epoch 191, Loss G: -0.3239, Loss D: 0.0407\n",
            "Epoch 192, Loss G: -0.5812, Loss D: -0.1741\n",
            "Epoch 193, Loss G: 0.0707, Loss D: -0.0405\n",
            "Epoch 194, Loss G: -0.1969, Loss D: -0.1251\n",
            "Epoch 195, Loss G: -0.1140, Loss D: -0.1503\n",
            "Epoch 196, Loss G: -0.2801, Loss D: -0.0927\n",
            "Epoch 197, Loss G: -0.5232, Loss D: -0.1789\n",
            "Epoch 198, Loss G: -0.4678, Loss D: -0.0860\n",
            "Epoch 199, Loss G: -0.6214, Loss D: 0.1063\n",
            "Epoch 200, Loss G: -0.3970, Loss D: -0.0586\n",
            "Epoch 201, Loss G: -0.4041, Loss D: 0.0721\n",
            "Epoch 202, Loss G: -0.3297, Loss D: -0.0024\n",
            "Epoch 203, Loss G: -0.3725, Loss D: -0.2777\n",
            "Epoch 204, Loss G: -0.2199, Loss D: -0.0863\n",
            "Epoch 205, Loss G: -0.4622, Loss D: -0.1209\n",
            "Epoch 206, Loss G: -0.4801, Loss D: -0.0673\n",
            "Epoch 207, Loss G: -0.6228, Loss D: -0.0623\n",
            "Epoch 208, Loss G: -0.3818, Loss D: -0.0290\n",
            "Epoch 209, Loss G: -0.3627, Loss D: -0.0429\n",
            "Epoch 210, Loss G: -0.5902, Loss D: 0.0120\n",
            "Epoch 211, Loss G: -0.3418, Loss D: -0.0662\n",
            "Epoch 212, Loss G: -0.3394, Loss D: -0.0298\n",
            "Epoch 213, Loss G: -0.2297, Loss D: -0.1210\n",
            "Epoch 214, Loss G: -0.7209, Loss D: -0.1164\n",
            "Epoch 215, Loss G: -0.2817, Loss D: 0.0182\n",
            "Epoch 216, Loss G: -0.2590, Loss D: 0.0964\n",
            "Epoch 217, Loss G: -0.4174, Loss D: 0.0548\n",
            "Epoch 218, Loss G: -0.3213, Loss D: -0.2427\n",
            "Epoch 219, Loss G: -0.3344, Loss D: -0.1464\n",
            "Epoch 220, Loss G: -0.1434, Loss D: -0.0299\n",
            "Epoch 221, Loss G: -0.1483, Loss D: -0.2352\n",
            "Epoch 222, Loss G: -0.4622, Loss D: 0.1025\n",
            "Epoch 223, Loss G: -0.3728, Loss D: -0.0450\n",
            "Epoch 224, Loss G: -0.4072, Loss D: -0.0366\n",
            "Epoch 225, Loss G: -0.5934, Loss D: -0.1306\n",
            "Epoch 226, Loss G: -0.4652, Loss D: -0.0039\n",
            "Epoch 227, Loss G: -0.3663, Loss D: -0.1000\n",
            "Epoch 228, Loss G: -0.3693, Loss D: -0.2196\n",
            "Epoch 229, Loss G: -0.6938, Loss D: -0.0789\n",
            "Epoch 230, Loss G: -0.7204, Loss D: -0.1229\n",
            "Epoch 231, Loss G: -0.6922, Loss D: -0.0533\n",
            "Epoch 232, Loss G: -0.7404, Loss D: -0.1427\n",
            "Epoch 233, Loss G: -0.6136, Loss D: 0.0819\n",
            "Epoch 234, Loss G: -0.8199, Loss D: -0.0183\n",
            "Epoch 235, Loss G: -0.4305, Loss D: -0.1544\n",
            "Epoch 236, Loss G: -0.4823, Loss D: -0.0613\n",
            "Epoch 237, Loss G: -0.8579, Loss D: -0.0953\n",
            "Epoch 238, Loss G: -0.6431, Loss D: -0.1180\n",
            "Epoch 239, Loss G: -0.5386, Loss D: -0.1458\n",
            "Epoch 240, Loss G: -1.1615, Loss D: -0.0667\n",
            "Epoch 241, Loss G: -0.6497, Loss D: -0.1020\n",
            "Epoch 242, Loss G: -0.5562, Loss D: -0.1502\n",
            "Epoch 243, Loss G: -0.1225, Loss D: -0.0698\n",
            "Epoch 244, Loss G: -0.4890, Loss D: -0.1747\n",
            "Epoch 245, Loss G: -1.0113, Loss D: 0.0369\n",
            "Epoch 246, Loss G: -0.2657, Loss D: -0.1301\n",
            "Epoch 247, Loss G: -0.7631, Loss D: -0.1341\n",
            "Epoch 248, Loss G: -0.3969, Loss D: 0.0580\n",
            "Epoch 249, Loss G: -0.3892, Loss D: -0.0105\n",
            "Epoch 250, Loss G: -0.5826, Loss D: -0.2106\n",
            "Epoch 251, Loss G: -0.4372, Loss D: -0.2306\n",
            "Epoch 252, Loss G: -0.3599, Loss D: -0.0920\n",
            "Epoch 253, Loss G: -0.5337, Loss D: 0.1320\n",
            "Epoch 254, Loss G: -0.1474, Loss D: -0.2510\n",
            "Epoch 255, Loss G: -0.4369, Loss D: -0.0035\n",
            "Epoch 256, Loss G: -0.1778, Loss D: -0.1059\n",
            "Epoch 257, Loss G: -0.5842, Loss D: -0.2492\n",
            "Epoch 258, Loss G: -0.2966, Loss D: -0.1047\n",
            "Epoch 259, Loss G: -0.5896, Loss D: -0.1371\n",
            "Epoch 260, Loss G: -0.6334, Loss D: -0.2013\n",
            "Epoch 261, Loss G: -0.6146, Loss D: 0.1269\n",
            "Epoch 262, Loss G: -0.6278, Loss D: -0.0725\n",
            "Epoch 263, Loss G: -0.0843, Loss D: 0.1185\n",
            "Epoch 264, Loss G: -0.4366, Loss D: -0.0607\n",
            "Epoch 265, Loss G: -0.5148, Loss D: -0.1789\n",
            "Epoch 266, Loss G: -0.1809, Loss D: 0.0605\n",
            "Epoch 267, Loss G: -0.3635, Loss D: -0.0334\n",
            "Epoch 268, Loss G: -0.3287, Loss D: 0.1591\n",
            "Epoch 269, Loss G: -0.1447, Loss D: -0.0153\n",
            "Epoch 270, Loss G: -0.2781, Loss D: -0.0412\n",
            "Epoch 271, Loss G: -0.4441, Loss D: -0.1330\n",
            "Epoch 272, Loss G: -0.3473, Loss D: -0.1177\n",
            "Epoch 273, Loss G: -0.4608, Loss D: 0.1152\n",
            "Epoch 274, Loss G: -0.5153, Loss D: -0.0074\n",
            "Epoch 275, Loss G: -0.1405, Loss D: -0.0109\n",
            "Epoch 276, Loss G: -0.3978, Loss D: -0.0950\n",
            "Epoch 277, Loss G: -0.4442, Loss D: -0.1800\n",
            "Epoch 278, Loss G: -0.2088, Loss D: 0.0838\n",
            "Epoch 279, Loss G: -0.2921, Loss D: -0.0948\n",
            "Epoch 280, Loss G: -0.2858, Loss D: -0.3631\n",
            "Epoch 281, Loss G: -0.2602, Loss D: -0.0615\n",
            "Epoch 282, Loss G: -0.2296, Loss D: -0.1208\n",
            "Epoch 283, Loss G: -0.4927, Loss D: -0.0493\n",
            "Epoch 284, Loss G: -0.0418, Loss D: -0.0550\n",
            "Epoch 285, Loss G: -0.3866, Loss D: -0.2535\n",
            "Epoch 286, Loss G: -0.3999, Loss D: 0.0953\n",
            "Epoch 287, Loss G: -0.3427, Loss D: 0.1475\n",
            "Epoch 288, Loss G: -0.4528, Loss D: -0.2126\n",
            "Epoch 289, Loss G: -0.3896, Loss D: -0.2260\n",
            "Epoch 290, Loss G: -0.3666, Loss D: -0.1302\n",
            "Epoch 291, Loss G: -0.2216, Loss D: -0.0798\n",
            "Epoch 292, Loss G: -0.6518, Loss D: 0.0237\n",
            "Epoch 293, Loss G: -0.2507, Loss D: -0.1321\n",
            "Epoch 294, Loss G: -0.5164, Loss D: -0.0337\n",
            "Epoch 295, Loss G: -0.3169, Loss D: 0.0512\n",
            "Epoch 296, Loss G: -0.5472, Loss D: -0.1712\n",
            "Epoch 297, Loss G: 0.2823, Loss D: -0.1234\n",
            "Epoch 298, Loss G: 0.1255, Loss D: -0.2807\n",
            "Epoch 299, Loss G: -0.3207, Loss D: -0.1998\n",
            "Epoch 300, Loss G: -0.2898, Loss D: -0.1510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g859p8XGg710"
      },
      "source": [
        "samples = ctgan.sample(800000)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PxvkVrOo3Yb",
        "outputId": "5a2a76bc-b49b-46e4-a4ec-d76e3b160eda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(samples)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuSVrHfxo6EJ",
        "outputId": "e255497a-7f75-4e65-f86c-796ec694f44d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "samples.columns"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Expense', 'Income', 'Loan type', 'Occupation type', 'Age',\n",
              "       'Score1', 'Score2', 'Score3', 'Score4', 'Score5', 'Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsEYS0ERpP7x"
      },
      "source": [
        "gen_data = np.array(samples)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B97G7jyupVvJ",
        "outputId": "80ef504d-2b77-4750-c514-16e114a4a0d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gen_data"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.65912882e+04, 1.90218051e+03, 1.66526253e+04, ...,\n",
              "        5.98447215e+02, 3.51289299e+03, 1.00000000e+00],\n",
              "       [7.51844413e+04, 1.63615109e+03, 1.76634197e+04, ...,\n",
              "        6.08907669e+02, 3.38332988e+03, 0.00000000e+00],\n",
              "       [6.48047120e+04, 1.77766504e+03, 1.51112009e+04, ...,\n",
              "        6.06691986e+02, 3.43408220e+03, 0.00000000e+00],\n",
              "       ...,\n",
              "       [7.90523184e+04, 1.92463253e+03, 1.68270980e+04, ...,\n",
              "        6.01198251e+02, 3.50076056e+03, 1.00000000e+00],\n",
              "       [7.44153812e+04, 1.72232514e+03, 1.63215914e+04, ...,\n",
              "        5.98602569e+02, 3.43779544e+03, 0.00000000e+00],\n",
              "       [4.26125102e+04, 1.82629605e+03, 1.28306557e+04, ...,\n",
              "        5.96298682e+02, 3.47288638e+03, 0.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6qquRW3pYkY"
      },
      "source": [
        "np.save('ctgan_8L_data', gen_data)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K-JKoOepftZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}